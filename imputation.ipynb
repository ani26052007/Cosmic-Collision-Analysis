{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a91b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import miceforest as mf\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e0dc0",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e1843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "X = data.drop(columns=['hazardous'])\n",
    "Y = data['hazardous']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    Y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(X.shape,Y.shape)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b60b86",
   "metadata": {},
   "source": [
    "### Getting to know the data better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c0536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provides a concise summary, including the number of entries, column names, and data types\n",
    "print(X_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52038b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Let us look at percentage of missing values per column\")\n",
    "# 1. Identify and Quantify\n",
    "missing_counts = X_train.isnull().sum()\n",
    "missing_percentage = (X_train.isnull().mean() * 100)\n",
    "\n",
    "# Combine into a summary table for easy viewing\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Percentage (%)': missing_percentage,\n",
    "    'dtype': X_train.dtypes\n",
    "}).sort_values(by='Percentage (%)', ascending=False)\n",
    "\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787dbe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Let us look at percentage of missing values test data per column\")\n",
    "# 1. Identify and Quantify\n",
    "missing_counts = X_test.isnull().sum()\n",
    "missing_percentage = (X_test.isnull().mean() * 100)\n",
    "\n",
    "# Combine into a summary table for easy viewing\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Percentage (%)': missing_percentage,\n",
    "    'dtype': X_test.dtypes\n",
    "}).sort_values(by='Percentage (%)', ascending=False)\n",
    "\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e49547",
   "metadata": {},
   "source": [
    "1. also miss_dist_astronomical, miss_dist_miles, miss_dist_lunar, miss_dist_kilometers tell about same thing. I am planning to use ***miss_dist_astronomical*** bcz it is only 13.38% empty and otheres are too much empty. if needed i will conver this to other units.\n",
    "2. miles_per_hour and relative_velocity_km_per_hr tell the same thing. I am planning to use ***miles_per_hour*** as it is only 19.826518% empty copmare to 32% of relative_velocity_km_per_hr. if needed i will conver this to other units\n",
    "Now let us drop these columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53348189",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop_ = ['miss_dist_miles', 'miss_dist_kilometers', 'miss_dist_lunar', 'relative_velocity_km_per_hr' ]\n",
    "X_train =  X_train.drop(columns = columns_to_drop_)\n",
    "X_test =  X_test.drop(columns = columns_to_drop_)\n",
    "\n",
    "missing_counts_1 = X_train.isnull().sum()\n",
    "missing_percentage_1 = (X_train.isnull().mean() * 100)\n",
    "\n",
    "# Combine into a summary table for easy viewing\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Count': missing_counts_1,\n",
    "    'Percentage (%)': missing_percentage_1,\n",
    "    'dtype': X_train.dtypes\n",
    "}).sort_values(by='Percentage (%)', ascending=False)\n",
    "\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5308c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(X_train.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title(\"Pattern of Missing Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ea31a",
   "metadata": {},
   "source": [
    "Now we have to fill these empty values. \n",
    "1. Strong Physics Relationships \n",
    "- mean_motion ↔ semi_major_axis (Kepler's 3rd Law)\n",
    "- aphelion_dist ↔ semi_major_axis + eccentricity\n",
    "- jupiter_tisserand_invariant (from semi_major_axis + eccentricity)\n",
    "2. Moderate Physics Relationships \n",
    "- Angular parameters (asc_node_longitude, perihelion_arg, mean_anomaly) - use circular statistics\n",
    "- miss_dist_astronomical - might correlate with orbital characteristics\n",
    "3. Temporal/Metadata \n",
    "- perihelion_time, epoch_osculation, epoch_date_close_approach - temporal relationships\n",
    "- approach_year, approach_month, approach_day - observation metadata\n",
    "4. Categorical \n",
    "- orbit_uncertainity\n",
    "- orbital_period\n",
    "- relative_velocity_km_per_sec - statistical methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802d9228",
   "metadata": {},
   "source": [
    "My stratergy to fill the Data: \n",
    "#### Physics-Based\n",
    "1. Semi Major Axis - Mean Motion (Kepler's 3rd Law): If you have one, you can calculate the other exactly\n",
    "2. Aphelion - Semi Major Axis - Eccentricity:\n",
    "a = (aphelion + perihelion) / 2\n",
    "e = (aphelion / a) - 1\n",
    "3. Jupiter Tisserand Invariant:\n",
    "J = (a_jupiter/a) + 2√(a/a_jupiter × (1-e²)) × cos(i)\n",
    "we dont have inclination (i). We can assume i = 0 as most NEOs have low inclination. We can find this only if we have semi_major_axis + eccentricity. else we can use MICE\n",
    "\n",
    "#### Angular Parameters (Circular Statistics) \n",
    "4. Asc Node Longitude, Perihelion Arg, Mean Anomaly:\n",
    "- These are angular/cyclic variables\n",
    "- Physics doesn't help much here - these vary widely for NEOs\n",
    "- I am going to conver them to sin and cos and then drop the original columns and then use MICE\n",
    "\n",
    "#### Temporal Metadata\n",
    "5. Epoch Osculation, Perihelion Time, Epoch Date Close Approach:\n",
    "- These are observation timestamps, not orbital properties\n",
    "\n",
    "6. Approach Year/Month/Day:\n",
    "- Pure metadata - when the asteroid was observed approaching Earth\n",
    "\n",
    "#### Distance & Velocity (Independent Measurements)\n",
    "7. Miss Distance (Astronomical)\n",
    "- This is the closest approach distance - varies wildly between asteroids\n",
    "- No physics formula to derive it from orbital elements alone\n",
    "- MICE (correlates with velocity, orbital period)\n",
    "\n",
    "\n",
    "8. Miles per Hour\n",
    "- Relative velocity at closest approach\n",
    "- Depends on both Earth's and asteroid's velocity vectors\n",
    "- Can't derive from orbital elements alone (need trajectory)\n",
    "- MICE (correlates with semi_major_axis, miss_distance)\n",
    "\n",
    "#### Categorical Variables\n",
    "9. Relative Velocity km per sec:\n",
    "- i think if we have miles_per_hour, you can categorize it\n",
    "\n",
    "10. Orbital Period\n",
    "- Categorical version of the actual orbital period\n",
    "\n",
    "11. Orbit Uncertainty\n",
    "- No way to derive this\n",
    "- New \"Unknown\" category as missing info might be usefull\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457082ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def physics_based_imputation_v2(df):\n",
    "    \"\"\"\n",
    "    Fill ONLY the physics-constrained relationships\n",
    "    Leave the rest for MICE\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Constants\n",
    "    AU = 1.496e11\n",
    "    G = 6.67430e-11\n",
    "    M_SUN = 1.989e30\n",
    "    \n",
    "    # 1. Semi-major axis ↔ Mean motion (Kepler's 3rd Law)\n",
    "    mask = df['mean_motion'].isna() & df['semi_major_axis'].notna()\n",
    "    if mask.sum() > 0:\n",
    "        a_m = df.loc[mask, 'semi_major_axis'] * AU\n",
    "        n_rad_per_sec = np.sqrt(G * M_SUN / a_m**3)\n",
    "        df.loc[mask, 'mean_motion'] = n_rad_per_sec * (180/np.pi) * 86400\n",
    "        print(f\"✓ Physics: Filled {mask.sum()} mean_motion from semi_major_axis\")\n",
    "    \n",
    "    mask = df['semi_major_axis'].isna() & df['mean_motion'].notna()\n",
    "    if mask.sum() > 0:\n",
    "        n_rad_per_sec = df.loc[mask, 'mean_motion'] * (np.pi/180) / 86400\n",
    "        a_m = (G * M_SUN / n_rad_per_sec**2)**(1/3)\n",
    "        df.loc[mask, 'semi_major_axis'] = a_m / AU\n",
    "        print(f\"✓ Physics: Filled {mask.sum()} semi_major_axis from mean_motion\")\n",
    "    \n",
    "    # 2. Calculate eccentricity where possible\n",
    "    mask = df['aphelion_dist'].notna() & df['semi_major_axis'].notna()\n",
    "    if mask.sum() > 0:\n",
    "        if 'eccentricity' not in df.columns:\n",
    "            df['eccentricity'] = np.nan\n",
    "        df.loc[mask, 'eccentricity'] = (df.loc[mask, 'aphelion_dist'] / \n",
    "                                         df.loc[mask, 'semi_major_axis']) - 1\n",
    "        print(f\"✓ Physics: Calculated {mask.sum()} eccentricity values\")\n",
    "    \n",
    "    # 3. Fill aphelion using eccentricity\n",
    "    if 'eccentricity' in df.columns:\n",
    "        mask = (df['aphelion_dist'].isna() & \n",
    "                df['semi_major_axis'].notna() & \n",
    "                df['eccentricity'].notna())\n",
    "        if mask.sum() > 0:\n",
    "            df.loc[mask, 'aphelion_dist'] = (df.loc[mask, 'semi_major_axis'] * \n",
    "                                              (1 + df.loc[mask, 'eccentricity']))\n",
    "            print(f\"✓ Physics: Filled {mask.sum()} aphelion_dist\")\n",
    "    \n",
    "    # 4. Temporal features - simple forward fill\n",
    "    temporal_cols = ['epoch_osculation', 'perihelion_time', 'epoch_date_close_approach']\n",
    "    for col in temporal_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    for col in ['approach_year', 'approach_month', 'approach_day']:\n",
    "        if col in df.columns and df[col].isna().sum() > 0:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0] if len(df[col].mode()) > 0 else df[col].median())\n",
    "    \n",
    "    return df\n",
    "\n",
    "X_train = physics_based_imputation_v2(X_train)\n",
    "X_test = physics_based_imputation_v2(X_test)\n",
    "\n",
    "# ============================================\n",
    "# STEP 4: Angular Features → Sin/Cos (BEFORE MICE)\n",
    "# ============================================\n",
    "# ChatGPT is right: Transform FIRST, then impute\n",
    "\n",
    "def convert_angles_to_circular(df):\n",
    "    \"\"\"Convert angular features to sin/cos representation\"\"\"\n",
    "    angular_cols = ['asc_node_longitude', 'perihelion_arg', 'mean_anomaly']\n",
    "    \n",
    "    for col in angular_cols:\n",
    "        if col in df.columns:\n",
    "            radians = np.deg2rad(df[col])\n",
    "            df[f'{col}_sin'] = np.sin(radians)\n",
    "            df[f'{col}_cos'] = np.cos(radians)\n",
    "    \n",
    "    # Drop original degree columns\n",
    "    df = df.drop(columns=[c for c in angular_cols if c in df.columns])\n",
    "    \n",
    "    return df\n",
    "\n",
    "X_train = convert_angles_to_circular(X_train)\n",
    "X_test = convert_angles_to_circular(X_test)\n",
    "\n",
    "print(\"\\n✓ Converted angular features to sin/cos\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 5: Handle Categoricals\n",
    "# ============================================\n",
    "# ChatGPT's advice: Missing as its own category\n",
    "\n",
    "categorical_cols = ['relative_velocity_km_per_sec', 'orbital_period', 'orbit_uncertainity']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in X_train.columns:\n",
    "        # Fill missing with \"Unknown\" category\n",
    "        X_train[col] = X_train[col].fillna('Unknown')\n",
    "        X_test[col] = X_test[col].fillna('Unknown')\n",
    "        \n",
    "        # Convert to category type for MICE\n",
    "        X_train[col] = X_train[col].astype('category')\n",
    "        X_test[col] = X_test[col].astype('category')\n",
    "\n",
    "print(\"✓ Handled categorical features\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 6: MICE for Remaining Features\n",
    "# ============================================\n",
    "# Now MICE will impute:\n",
    "# - sin/cos components (preserving correlations!)\n",
    "# - miss_dist_astronomical (correlated with velocity, hazard)\n",
    "# - miles_per_hour (correlated with orbit, hazard)\n",
    "# - jupiter_tisserand_invariant (if still missing)\n",
    "\n",
    "print(\"\\nRemaining missing values before MICE:\")\n",
    "missing = X_train.isna().sum()\n",
    "print(missing[missing > 0])\n",
    "\n",
    "# Reset index for MICE\n",
    "X_train_mice = X_train.copy().reset_index(drop=True)\n",
    "X_test_mice = X_test.copy().reset_index(drop=True)\n",
    "\n",
    "# Fit MICE on training data\n",
    "print(\"\\nFitting MICE on training data...\")\n",
    "kds = mf.ImputationKernel(\n",
    "    X_train_mice,\n",
    "    random_state=42,\n",
    "    mean_match_candidates=5  # Prevents KDTree errors\n",
    ")\n",
    "\n",
    "kds.mice(iterations=5, verbose=True)\n",
    "X_train_imputed = kds.complete_data()\n",
    "\n",
    "# Apply to test data\n",
    "print(\"\\nImputing test data...\")\n",
    "X_test_imputed = kds.impute_new_data(X_test_mice).complete_data()\n",
    "\n",
    "# ============================================\n",
    "# STEP 7: Post-MICE Renormalization (Optional)\n",
    "# ============================================\n",
    "# ChatGPT suggested this: Enforce sin² + cos² = 1\n",
    "\n",
    "def renormalize_circular(df):\n",
    "    \"\"\"Ensure sin² + cos² = 1 for circular features\"\"\"\n",
    "    angular_bases = ['asc_node_longitude', 'perihelion_arg', 'mean_anomaly']\n",
    "    \n",
    "    for base in angular_bases:\n",
    "        sin_col = f'{base}_sin'\n",
    "        cos_col = f'{base}_cos'\n",
    "        \n",
    "        if sin_col in df.columns and cos_col in df.columns:\n",
    "            # Calculate magnitude\n",
    "            magnitude = np.sqrt(df[sin_col]**2 + df[cos_col]**2)\n",
    "            \n",
    "            # Renormalize\n",
    "            df[sin_col] = df[sin_col] / magnitude\n",
    "            df[cos_col] = df[cos_col] / magnitude\n",
    "    \n",
    "    return df\n",
    "\n",
    "X_train_final = renormalize_circular(X_train_imputed)\n",
    "X_test_final = renormalize_circular(X_test_imputed)\n",
    "\n",
    "print(\"\\n✓ Renormalized circular features\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 8: Validation\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check no missing values\n",
    "print(f\"\\nMissing in train: {X_train_final.isna().sum().sum()}\")\n",
    "print(f\"Missing in test: {X_test_final.isna().sum().sum()}\")\n",
    "\n",
    "# Validate Kepler's 3rd Law still holds\n",
    "AU = 1.496e11\n",
    "G = 6.67430e-11\n",
    "M_SUN = 1.989e30\n",
    "\n",
    "mask = X_train_final['semi_major_axis'].notna() & X_train_final['mean_motion'].notna()\n",
    "if mask.sum() > 0:\n",
    "    a_m = X_train_final.loc[mask, 'semi_major_axis'] * AU\n",
    "    n_calc = np.sqrt(G * M_SUN / a_m**3) * (180/np.pi) * 86400\n",
    "    n_actual = X_train_final.loc[mask, 'mean_motion']\n",
    "    \n",
    "    error = np.abs(n_calc - n_actual) / n_actual * 100\n",
    "    print(f\"\\nKepler's Law validation:\")\n",
    "    print(f\"  Mean error: {error.mean():.4f}%\")\n",
    "    print(f\"  Max error: {error.max():.4f}%\")\n",
    "\n",
    "# Validate circular features\n",
    "print(\"\\nCircular feature validation (sin² + cos² = 1):\")\n",
    "for base in ['asc_node_longitude', 'perihelion_arg', 'mean_anomaly']:\n",
    "    sin_col = f'{base}_sin'\n",
    "    cos_col = f'{base}_cos'\n",
    "    \n",
    "    if sin_col in X_train_final.columns:\n",
    "        magnitude = X_train_final[sin_col]**2 + X_train_final[cos_col]**2\n",
    "        error = np.abs(magnitude - 1)\n",
    "        print(f\"  {base}: max error = {error.max():.2e}\")\n",
    "\n",
    "print(\"\\n✅ Data ready for modeling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "AU = 1.496e11\n",
    "G = 6.67430e-11\n",
    "M_SUN = 1.989e30\n",
    "\n",
    "# Calculate what mean_motion SHOULD be\n",
    "a_m = X_train_final['semi_major_axis'] * AU\n",
    "n_calculated = np.sqrt(G * M_SUN / a_m**3) * (180/np.pi) * 86400\n",
    "n_actual = X_train_final['mean_motion']\n",
    "\n",
    "# Calculate error\n",
    "relative_error = np.abs(n_calculated - n_actual) / n_actual * 100\n",
    "\n",
    "# Find the worst offenders\n",
    "worst_indices = relative_error.nlargest(10).index\n",
    "\n",
    "print(\"Top 10 asteroids violating Kepler's 3rd Law:\")\n",
    "print(\"=\"*80)\n",
    "suspect_df = pd.DataFrame({\n",
    "    'semi_major_axis': X_train_final.loc[worst_indices, 'semi_major_axis'],\n",
    "    'mean_motion_actual': n_actual.loc[worst_indices],\n",
    "    'mean_motion_expected': n_calculated.loc[worst_indices],\n",
    "    'error_%': relative_error.loc[worst_indices]\n",
    "})\n",
    "print(suspect_df)\n",
    "\n",
    "# Check if these were imputed or original\n",
    "print(\"\\nWere these values imputed by MICE?\")\n",
    "print(\"(Check if they had missing semi_major_axis or mean_motion originally)\")\n",
    "\n",
    "# Distribution of errors\n",
    "print(\"\\nError distribution:\")\n",
    "print(f\"< 5% error: {(relative_error < 5).sum()} asteroids ({(relative_error < 5).sum()/len(relative_error)*100:.1f}%)\")\n",
    "print(f\"5-10% error: {((relative_error >= 5) & (relative_error < 10)).sum()} asteroids\")\n",
    "print(f\"10-20% error: {((relative_error >= 10) & (relative_error < 20)).sum()} asteroids\")\n",
    "print(f\"> 20% error: {(relative_error >= 20).sum()} asteroids ({(relative_error >= 20).sum()/len(relative_error)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e1b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which asteroids originally had missing values\n",
    "# (You'll need to run this on your ORIGINAL data before imputation)\n",
    "\n",
    "# Load original data again\n",
    "data_original = pd.read_csv('train.csv')\n",
    "X_original = data_original.drop(columns=['hazardous'])\n",
    "\n",
    "X_train_original, _, _, _ = train_test_split(\n",
    "    X_original, data_original['hazardous'], \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=data_original['hazardous']\n",
    ")\n",
    "\n",
    "# Check the problem indices\n",
    "problem_indices = [2104, 1609, 391, 466, 102, 2819, 2587, 2559, 2879]\n",
    "\n",
    "print(\"Did these asteroids have missing values originally?\")\n",
    "print(\"=\"*80)\n",
    "for idx in problem_indices:\n",
    "    sma_missing = pd.isna(X_train_original.loc[idx, 'semi_major_axis'])\n",
    "    mm_missing = pd.isna(X_train_original.loc[idx, 'mean_motion'])\n",
    "    print(f\"Index {idx}: semi_major_axis missing={sma_missing}, mean_motion missing={mm_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c2bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================\n",
    "# STEP 1: CREATE VIOLATION FEATURE (OPTIONAL)\n",
    "# ============================================\n",
    "\n",
    "def add_kepler_violation_feature(df):\n",
    "    \"\"\"\n",
    "    Capture magnitude of Kepler's law violation\n",
    "    This might indicate measurement uncertainty\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    AU = 1.496e11\n",
    "    G = 6.67430e-11\n",
    "    M_SUN = 1.989e30\n",
    "    \n",
    "    a_m = df['semi_major_axis'] * AU\n",
    "    n_expected = np.sqrt(G * M_SUN / a_m**3) * (180/np.pi) * 86400\n",
    "    \n",
    "    # Relative error as a feature\n",
    "    df['kepler_violation'] = np.abs(\n",
    "        df['mean_motion'] - n_expected\n",
    "    ) / df['mean_motion']\n",
    "    \n",
    "    print(f\"✓ Created kepler_violation feature\")\n",
    "    print(f\"  Mean violation: {df['kepler_violation'].mean()*100:.2f}%\")\n",
    "    print(f\"  Max violation: {df['kepler_violation'].max()*100:.2f}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "X_train_final = add_kepler_violation_feature(X_train_final)\n",
    "X_test_final = add_kepler_violation_feature(X_test_final)\n",
    "\n",
    "# ============================================\n",
    "# STEP 2: ENFORCE KEPLER'S LAW\n",
    "# ============================================\n",
    "\n",
    "def enforce_keplers_law(df):\n",
    "    \"\"\"\n",
    "    Recalculate mean_motion from semi_major_axis\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    AU = 1.496e11\n",
    "    G = 6.67430e-11\n",
    "    M_SUN = 1.989e30\n",
    "    \n",
    "    a_m = df['semi_major_axis'] * AU\n",
    "    df['mean_motion'] = np.sqrt(G * M_SUN / a_m**3) * (180/np.pi) * 86400\n",
    "    \n",
    "    return df\n",
    "\n",
    "X_train_final = enforce_keplers_law(X_train_final)\n",
    "X_test_final = enforce_keplers_law(X_test_final)\n",
    "\n",
    "print(\"\\n✓ Physics enforced: Kepler's 3rd Law now holds\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 3: VALIDATE\n",
    "# ============================================\n",
    "\n",
    "# Should now be perfect\n",
    "a_m = X_train_final['semi_major_axis'] * AU\n",
    "n_calc = np.sqrt(G * M_SUN / a_m**3) * (180/np.pi) * 86400\n",
    "error = np.abs(n_calc - X_train_final['mean_motion']) / X_train_final['mean_motion'] * 100\n",
    "\n",
    "print(f\"\\nValidation:\")\n",
    "print(f\"  Mean error: {error.mean():.2e}%\")  # Should be ~0\n",
    "print(f\"  Max error: {error.max():.2e}%\")    # Should be ~0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f751f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_final.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4032e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick visual check: Do hazardous asteroids have different orbital energies?\n",
    "mask_ = X_train_final['kepler_violation'] * 100 > 10\n",
    "X_train_final.loc[mask_, 'kepler_violation'] * 100\n",
    "violations = X_train_final.loc[\n",
    "    X_train_final['kepler_violation'] * 100 > 1,\n",
    "    'kepler_violation'\n",
    "] * 100\n",
    "print(len(X_train_final))\n",
    "print(violations)\n",
    "\n",
    "plt.scatter(X_train_final['kepler_violation'], y_train)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f558b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts_2 = X_train_final.isnull().sum()\n",
    "missing_percentage_2 = (X_train_final.isnull().mean() * 100)\n",
    "\n",
    "# Combine into a summary table for easy viewing\n",
    "missing_summary_2 = pd.DataFrame({\n",
    "    'Missing Count': missing_counts_2,\n",
    "    'Percentage (%)': missing_percentage_2,\n",
    "    'dtype': X_train_final.dtypes\n",
    "}).sort_values(by='Percentage (%)', ascending=False)\n",
    "\n",
    "print(missing_summary_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.value_counts())\n",
    "print(y_train.value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06653959",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.value_counts())\n",
    "print(y_test.value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9b5015",
   "metadata": {},
   "source": [
    "Let us first divide our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30be92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['relative_velocity_km_per_sec', 'orbit_uncertainity', 'orbital_period']\n",
    "X_train_num =  X_train.drop(columns = columns_to_drop)\n",
    "X_train_obj = X_train.select_dtypes(include=['object'])\n",
    "X_test_num =  X_test.drop(columns = columns_to_drop)\n",
    "X_test_obj = X_test.select_dtypes(include=['object'])\n",
    "print(\"only data with numbers\")\n",
    "print(X_train_num.info())\n",
    "print(\"only data with objects\")\n",
    "print(X_train_obj.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f741d929",
   "metadata": {},
   "source": [
    "Now let us also look at skewness of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness_vals = X_train_num.skew().sort_values(ascending=False)\n",
    "\n",
    "# Flag features that need transformation (> 0.5 or < -0.5)\n",
    "transform_needed = skewness_vals\n",
    "print(\"output of the .skew\")\n",
    "print(transform_needed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
